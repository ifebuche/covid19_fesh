""" parquet compat """

<<<<<<< HEAD
from typing import Any, AnyStr, Dict, List, Optional
from warnings import catch_warnings

from pandas._typing import FilePathOrBuffer
=======
from typing import Any, Dict, Optional
from warnings import catch_warnings

>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
from pandas.compat._optional import import_optional_dependency
from pandas.errors import AbstractMethodError

from pandas import DataFrame, get_option

<<<<<<< HEAD
from pandas.io.common import _expand_user, get_filepath_or_buffer, is_fsspec_url
=======
from pandas.io.common import get_filepath_or_buffer, is_gcs_url, is_s3_url
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef


def get_engine(engine: str) -> "BaseImpl":
    """ return our implementation """
<<<<<<< HEAD
=======

>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
    if engine == "auto":
        engine = get_option("io.parquet.engine")

    if engine == "auto":
        # try engines in this order
<<<<<<< HEAD
        engine_classes = [PyArrowImpl, FastParquetImpl]

        error_msgs = ""
        for engine_class in engine_classes:
            try:
                return engine_class()
            except ImportError as err:
                error_msgs += "\n - " + str(err)
=======
        try:
            return PyArrowImpl()
        except ImportError:
            pass

        try:
            return FastParquetImpl()
        except ImportError:
            pass
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef

        raise ImportError(
            "Unable to find a usable engine; "
            "tried using: 'pyarrow', 'fastparquet'.\n"
<<<<<<< HEAD
            "A suitable version of "
            "pyarrow or fastparquet is required for parquet "
            "support.\n"
            "Trying to import the above resulted in these errors:"
            f"{error_msgs}"
=======
            "pyarrow or fastparquet is required for parquet "
            "support"
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
        )

    if engine == "pyarrow":
        return PyArrowImpl()
    elif engine == "fastparquet":
        return FastParquetImpl()

    raise ValueError("engine must be one of 'pyarrow', 'fastparquet'")


class BaseImpl:
    @staticmethod
    def validate_dataframe(df: DataFrame):

        if not isinstance(df, DataFrame):
            raise ValueError("to_parquet only supports IO with DataFrames")

        # must have value column names (strings only)
<<<<<<< HEAD
        if df.columns.inferred_type not in {"string", "empty"}:
=======
        if df.columns.inferred_type not in {"string", "unicode", "empty"}:
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
            raise ValueError("parquet must have string column names")

        # index level names must be strings
        valid_names = all(
            isinstance(name, str) for name in df.index.names if name is not None
        )
        if not valid_names:
            raise ValueError("Index level names must be strings")

    def write(self, df: DataFrame, path, compression, **kwargs):
        raise AbstractMethodError(self)

    def read(self, path, columns=None, **kwargs):
        raise AbstractMethodError(self)


class PyArrowImpl(BaseImpl):
    def __init__(self):
        import_optional_dependency(
            "pyarrow", extra="pyarrow is required for parquet support."
        )
        import pyarrow.parquet

        # import utils to register the pyarrow extension types
        import pandas.core.arrays._arrow_utils  # noqa

        self.api = pyarrow

    def write(
        self,
        df: DataFrame,
<<<<<<< HEAD
        path: FilePathOrBuffer[AnyStr],
        compression: Optional[str] = "snappy",
        index: Optional[bool] = None,
        partition_cols: Optional[List[str]] = None,
        **kwargs,
    ):
        self.validate_dataframe(df)
=======
        path,
        compression="snappy",
        coerce_timestamps="ms",
        index: Optional[bool] = None,
        partition_cols=None,
        **kwargs,
    ):
        self.validate_dataframe(df)
        path, _, _, _ = get_filepath_or_buffer(path, mode="wb")
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef

        from_pandas_kwargs: Dict[str, Any] = {"schema": kwargs.pop("schema", None)}
        if index is not None:
            from_pandas_kwargs["preserve_index"] = index

        table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
<<<<<<< HEAD

        if is_fsspec_url(path) and "filesystem" not in kwargs:
            # make fsspec instance, which pyarrow will use to open paths
            import_optional_dependency("fsspec")
            import fsspec.core

            fs, path = fsspec.core.url_to_fs(path)
            kwargs["filesystem"] = fs
        else:
            path = _expand_user(path)
        if partition_cols is not None:
            # writes to multiple files under the given path
=======
        if partition_cols is not None:
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
            self.api.parquet.write_to_dataset(
                table,
                path,
                compression=compression,
<<<<<<< HEAD
=======
                coerce_timestamps=coerce_timestamps,
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
                partition_cols=partition_cols,
                **kwargs,
            )
        else:
<<<<<<< HEAD
            # write to single output file
            self.api.parquet.write_table(table, path, compression=compression, **kwargs)

    def read(self, path, columns=None, **kwargs):
        if is_fsspec_url(path) and "filesystem" not in kwargs:
            import_optional_dependency("fsspec")
            import fsspec.core

            fs, path = fsspec.core.url_to_fs(path)
            should_close = False
        else:
            fs = kwargs.pop("filesystem", None)
            should_close = False
            path = _expand_user(path)

        if not fs:
            path, _, _, should_close = get_filepath_or_buffer(path)

        kwargs["use_pandas_metadata"] = True
        result = self.api.parquet.read_table(
            path, columns=columns, filesystem=fs, **kwargs
=======
            self.api.parquet.write_table(
                table,
                path,
                compression=compression,
                coerce_timestamps=coerce_timestamps,
                **kwargs,
            )

    def read(self, path, columns=None, **kwargs):
        path, _, _, should_close = get_filepath_or_buffer(path)

        kwargs["use_pandas_metadata"] = True
        result = self.api.parquet.read_table(
            path, columns=columns, **kwargs
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
        ).to_pandas()
        if should_close:
            path.close()

        return result


class FastParquetImpl(BaseImpl):
    def __init__(self):
        # since pandas is a dependency of fastparquet
        # we need to import on first use
        fastparquet = import_optional_dependency(
            "fastparquet", extra="fastparquet is required for parquet support."
        )
        self.api = fastparquet

    def write(
        self,
        df: DataFrame,
        path,
        compression="snappy",
        index=None,
        partition_cols=None,
        **kwargs,
    ):
        self.validate_dataframe(df)
        # thriftpy/protocol/compact.py:339:
        # DeprecationWarning: tostring() is deprecated.
        # Use tobytes() instead.

        if "partition_on" in kwargs and partition_cols is not None:
            raise ValueError(
                "Cannot use both partition_on and "
<<<<<<< HEAD
                "partition_cols. Use partition_cols for partitioning data"
=======
                "partition_cols. Use partition_cols for "
                "partitioning data"
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
            )
        elif "partition_on" in kwargs:
            partition_cols = kwargs.pop("partition_on")

        if partition_cols is not None:
            kwargs["file_scheme"] = "hive"

<<<<<<< HEAD
        if is_fsspec_url(path):
            fsspec = import_optional_dependency("fsspec")

            # if filesystem is provided by fsspec, file must be opened in 'wb' mode.
            kwargs["open_with"] = lambda path, _: fsspec.open(path, "wb").open()
=======
        if is_s3_url(path) or is_gcs_url(path):
            # if path is s3:// or gs:// we need to open the file in 'wb' mode.
            # TODO: Support 'ab'

            path, _, _, _ = get_filepath_or_buffer(path, mode="wb")
            # And pass the opened file to the fastparquet internal impl.
            kwargs["open_with"] = lambda path, _: path
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
        else:
            path, _, _, _ = get_filepath_or_buffer(path)

        with catch_warnings(record=True):
            self.api.write(
                path,
                df,
                compression=compression,
                write_index=index,
                partition_on=partition_cols,
                **kwargs,
            )

    def read(self, path, columns=None, **kwargs):
<<<<<<< HEAD
        if is_fsspec_url(path):
            fsspec = import_optional_dependency("fsspec")

            open_with = lambda path, _: fsspec.open(path, "rb").open()
            parquet_file = self.api.ParquetFile(path, open_with=open_with)
=======
        if is_s3_url(path):
            from pandas.io.s3 import get_file_and_filesystem

            # When path is s3:// an S3File is returned.
            # We need to retain the original path(str) while also
            # pass the S3File().open function to fsatparquet impl.
            s3, filesystem = get_file_and_filesystem(path)
            try:
                parquet_file = self.api.ParquetFile(path, open_with=filesystem.open)
            finally:
                s3.close()
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
        else:
            path, _, _, _ = get_filepath_or_buffer(path)
            parquet_file = self.api.ParquetFile(path)

        return parquet_file.to_pandas(columns=columns, **kwargs)


def to_parquet(
    df: DataFrame,
<<<<<<< HEAD
    path: FilePathOrBuffer[AnyStr],
    engine: str = "auto",
    compression: Optional[str] = "snappy",
    index: Optional[bool] = None,
    partition_cols: Optional[List[str]] = None,
=======
    path,
    engine: str = "auto",
    compression="snappy",
    index: Optional[bool] = None,
    partition_cols=None,
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
    **kwargs,
):
    """
    Write a DataFrame to the parquet format.

    Parameters
    ----------
    df : DataFrame
<<<<<<< HEAD
    path : str or file-like object
        If a string, it will be used as Root Directory path
        when writing a partitioned dataset. By file-like object,
        we refer to objects with a write() method, such as a file handler
        (e.g. via builtin open function) or io.BytesIO. The engine
        fastparquet does not accept file-like objects.
=======
    path : str
        File path or Root Directory path. Will be used as Root Directory path
        while writing a partitioned dataset.
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef

        .. versionchanged:: 0.24.0

    engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'
        Parquet library to use. If 'auto', then the option
        ``io.parquet.engine`` is used. The default ``io.parquet.engine``
        behavior is to try 'pyarrow', falling back to 'fastparquet' if
        'pyarrow' is unavailable.
    compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'
        Name of the compression to use. Use ``None`` for no compression.
    index : bool, default None
        If ``True``, include the dataframe's index(es) in the file output. If
        ``False``, they will not be written to the file.
        If ``None``, similar to ``True`` the dataframe's index(es)
        will be saved. However, instead of being saved as values,
        the RangeIndex will be stored as a range in the metadata so it
        doesn't require much space and is faster. Other indexes will
        be included as columns in the file output.

        .. versionadded:: 0.24.0

    partition_cols : str or list, optional, default None
<<<<<<< HEAD
        Column names by which to partition the dataset.
        Columns are partitioned in the order they are given.
        Must be None if path is not a string.
=======
        Column names by which to partition the dataset
        Columns are partitioned in the order they are given
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef

        .. versionadded:: 0.24.0

    kwargs
        Additional keyword arguments passed to the engine
    """
    if isinstance(partition_cols, str):
        partition_cols = [partition_cols]
    impl = get_engine(engine)
    return impl.write(
        df,
        path,
        compression=compression,
        index=index,
        partition_cols=partition_cols,
        **kwargs,
    )


def read_parquet(path, engine: str = "auto", columns=None, **kwargs):
    """
    Load a parquet object from the file path, returning a DataFrame.

<<<<<<< HEAD
=======
    .. versionadded:: 0.21.0

>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
    Parameters
    ----------
    path : str, path object or file-like object
        Any valid string path is acceptable. The string could be a URL. Valid
        URL schemes include http, ftp, s3, and file. For file URLs, a host is
        expected. A local file could be:
        ``file://localhost/path/to/table.parquet``.
        A file URL can also be a path to a directory that contains multiple
        partitioned parquet files. Both pyarrow and fastparquet support
        paths to directories as well as file URLs. A directory path could be:
<<<<<<< HEAD
        ``file://localhost/path/to/tables`` or ``s3://bucket/partition_dir``
=======
        ``file://localhost/path/to/tables``
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef

        If you want to pass in a path object, pandas accepts any
        ``os.PathLike``.

        By file-like object, we refer to objects with a ``read()`` method,
        such as a file handler (e.g. via builtin ``open`` function)
        or ``StringIO``.
    engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'
        Parquet library to use. If 'auto', then the option
        ``io.parquet.engine`` is used. The default ``io.parquet.engine``
        behavior is to try 'pyarrow', falling back to 'fastparquet' if
        'pyarrow' is unavailable.
    columns : list, default=None
        If not None, only these columns will be read from the file.
<<<<<<< HEAD
=======

        .. versionadded:: 0.21.1
>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
    **kwargs
        Any additional kwargs are passed to the engine.

    Returns
    -------
    DataFrame
    """
<<<<<<< HEAD
=======

>>>>>>> 27dd9875f98c51b82553091a9dbcf027191524ef
    impl = get_engine(engine)
    return impl.read(path, columns=columns, **kwargs)
